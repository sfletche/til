Years ago I learned about robots.txt and promptly forgot about it.
Today I was reminded...

[Web crawlers](https://en.wikipedia.org/wiki/Web_crawler) are what search engines use to index the world wide web.
The resulting index can then be used to respond to web searches.  

A [`robots.txt`](https://en.wikipedia.org/wiki/Robots_exclusion_standard) file can be used to indicate which paths/files/etc the web crawlers should ignore.


